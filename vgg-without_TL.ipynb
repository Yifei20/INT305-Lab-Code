{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T13:38:59.875440Z","iopub.status.busy":"2023-12-08T13:38:59.875164Z","iopub.status.idle":"2023-12-08T13:39:04.443435Z","shell.execute_reply":"2023-12-08T13:39:04.442372Z","shell.execute_reply.started":"2023-12-08T13:38:59.875415Z"},"trusted":true},"outputs":[],"source":["from torchvision import datasets\n","from torchvision.transforms import transforms\n","from torch.utils.data import random_split\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision.models import vgg11\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import numpy as np\n","from tqdm import tqdm\n","\n","from sklearn.metrics import ConfusionMatrixDisplay\n","import warnings \n","warnings.simplefilter(action='ignore', category=FutureWarning) \n"]},{"cell_type":"markdown","metadata":{},"source":["## Helper Functions & Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T13:39:04.445994Z","iopub.status.busy":"2023-12-08T13:39:04.445504Z","iopub.status.idle":"2023-12-08T13:39:04.540088Z","shell.execute_reply":"2023-12-08T13:39:04.539127Z","shell.execute_reply.started":"2023-12-08T13:39:04.445958Z"},"trusted":true},"outputs":[],"source":["# Hyperparameters\n","valid_size = 0.2\n","batch_size = 10\n","lr=0.001\n","n_epochs = 100\n","\n","# Parameter Definition\n","num_workers = 0\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","              'dog', 'frog', 'horse', 'ship', 'truck']\n","mean = torch.tensor([0.485, 0.456, 0.406])\n","std = torch.tensor([0.229, 0.224, 0.225])\n","\n","random_seed = 42\n","torch.manual_seed(random_seed)\n","\n","# Define denormalizer\n","de_mean = [-mean/std for mean, std in zip(mean, std)]\n","de_std = [1/std for std in std]\n","denormalizer = transforms.Normalize(mean=de_mean, std=de_std)\n","\n","def spilt_train_valid(train_dataset, valid_set_size):\n","    valid_set_size = int(valid_set_size * len(train_dataset))\n","    train_set_size = len(train_dataset) - valid_set_size\n","    return random_split(train_dataset, [train_set_size, valid_set_size])\n","\n","def load_cifar10(is_train, transform):\n","    return datasets.CIFAR10(root='data', train=is_train, \n","                            transform=transform, download=True)\n","\n","def confusion_matrix(preds, labels, conf_matrix):\n","    for p, t in zip(preds, labels):\n","        conf_matrix[p, t] += 1\n","    return conf_matrix\n","\n","def plot_confusion_matrix(conf_matrix, class_names, normalize=False, ax_display=True, title='Confusion Matrix'):\n","    _, ax = plt.subplots(figsize=(8,6))\n","    if normalize:\n","        conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n","        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n","        disp.plot(cmap=plt.cm.Blues,ax=ax, xticks_rotation=45, values_format='.2f')\n","    else:\n","        conf_matrix = conf_matrix.astype('int')\n","        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n","        disp.plot(cmap=plt.cm.Blues,ax=ax, xticks_rotation=45, values_format='d')\n","    if ax_display is False:\n","        left, right = plt.xlim()\n","        ax.spines['left'].set_position(('data', left))\n","        ax.spines['right'].set_position(('data', right))\n","        for edge_i in ['top', 'bottom', 'right', 'left']:\n","            ax.spines[edge_i].set_edgecolor(\"white\")\n","    plt.title(title, fontdict={'size': 14})\n","    plt.tight_layout()\n","    plt.ylabel('True label', fontdict={'size': 14})\n","    plt.xlabel('Predicted label', fontdict={'size': 14})\n","    plt.show()\n","\n","def plot_class_samples(samples, preds, values, title='Examples'):\n","  plt.figure(figsize=(12, 6))\n","  for idx in range(10):\n","      plt.subplot(2, 5, idx+1)\n","      img = denormalizer(samples[idx]).cpu().numpy()\n","      plt.imshow(np.transpose(img, (1, 2, 0))) \n","      plt.title('True: {:s} (Pred: {:s})'.format(class_names[idx], class_names[preds[idx]]), \n","                fontdict={'size': 10},\n","                color=(\"green\" if preds[idx]==idx else \"red\"))\n","      plt.xlabel('Confidence value: {:.2f}'.format(values[idx]), \n","                fontdict={'size'   : 10})\n","      plt.yticks([])\n","      plt.xticks([])\n","  plt.suptitle(title, fontdict={'size': 14})\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Augumentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T13:39:04.541695Z","iopub.status.busy":"2023-12-08T13:39:04.541330Z","iopub.status.idle":"2023-12-08T13:39:04.548367Z","shell.execute_reply":"2023-12-08T13:39:04.547276Z","shell.execute_reply.started":"2023-12-08T13:39:04.541661Z"},"trusted":true},"outputs":[],"source":["train_transform = transforms.Compose([\n","                                transforms.RandomHorizontalFlip(),\n","                                transforms.RandomGrayscale(),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean, std)\n","                                ])\n","\n","test_transform = transforms.Compose([\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean, std)\n","                                ])"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T13:39:04.551608Z","iopub.status.busy":"2023-12-08T13:39:04.551276Z","iopub.status.idle":"2023-12-08T13:39:14.620911Z","shell.execute_reply":"2023-12-08T13:39:14.619811Z","shell.execute_reply.started":"2023-12-08T13:39:04.551576Z"},"trusted":true},"outputs":[],"source":["# Load train, validation and test datasets as iterators\n","# Data\n","train_ds = load_cifar10(is_train=True, transform=train_transform)\n","test_ds = load_cifar10(is_train=False, transform=test_transform)\n","train_ds, valid_ds = spilt_train_valid(train_ds, valid_size)\n","\n","train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, \n","                                         shuffle=True, num_workers=num_workers)\n","valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, \n","                                         shuffle=True, num_workers=num_workers)\n","test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size,\n","                                        shuffle=False, num_workers=num_workers)"]},{"cell_type":"markdown","metadata":{},"source":["## Network Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T13:39:14.635114Z","iopub.status.busy":"2023-12-08T13:39:14.634774Z","iopub.status.idle":"2023-12-08T13:39:19.868591Z","shell.execute_reply":"2023-12-08T13:39:19.867711Z","shell.execute_reply.started":"2023-12-08T13:39:14.635082Z"},"trusted":true},"outputs":[],"source":["model = vgg11()\n","model.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T13:39:19.870557Z","iopub.status.busy":"2023-12-08T13:39:19.870185Z","iopub.status.idle":"2023-12-08T14:16:07.615002Z","shell.execute_reply":"2023-12-08T14:16:07.613317Z","shell.execute_reply.started":"2023-12-08T13:39:19.870523Z"},"trusted":true},"outputs":[],"source":["valid_loss_min = np.Inf\n","train_epochs_loss = []\n","valid_epochs_loss = []\n","train_epochs_acc = []\n","valid_epochs_acc = []\n","\n","for epoch in range(1, n_epochs+1):\n","    # Record train/valid loss and acc for each epoch\n","    train_loss_sum = 0.0\n","    valid_loss_sum = 0.0\n","    train_acc_num = 0.0\n","    valid_acc_num = 0.0\n","    # Start training and validating\n","    # ========================= train model =====================\n","    print('======================== Epoch: {} ========================'.format(epoch))\n","    model.train()\n","    for images, labels in tqdm(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        logits = model(images)\n","        loss = loss_fn(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss_sum += loss.item() * images.size(0)\n","        train_acc_num += sum(torch.max(logits, dim=1)[1] == labels).cpu()\n","    train_acc = 100 * train_acc_num/len(train_loader.dataset)\n","    train_loss = train_loss_sum/len(train_loader.dataset)\n","    train_epochs_loss.append(train_loss)\n","    train_epochs_acc.append(train_acc)\n","    print('Train loss: {:.3f}, Train acc: {:.1f}%'.format(train_loss, train_acc))\n","\n","    # ========================= valid model =====================\n","    model.eval()\n","    for images, labels in tqdm(valid_loader):\n","        images, labels = images.to(device), labels.to(device)\n","        with torch.no_grad():\n","            logits = model(images)\n","            loss = loss_fn(logits, labels)\n","        valid_loss_sum += loss.item() * images.size(0)\n","        valid_acc_num += sum(torch.max(logits, dim=1)[1] == labels).cpu()\n","    valid_acc= 100 * valid_acc_num/len(valid_loader.dataset)\n","    valid_loss = valid_loss_sum/len(valid_loader.dataset)\n","    valid_epochs_loss.append(valid_loss)\n","    valid_epochs_acc.append(valid_acc)\n","    print('Valid loss: {:.3f}, Valid acc: {:.1f}%'.format(valid_loss, valid_acc))\n","\n","    # ========================= save model =====================\n","    # Save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.3f} --> {:.3f}).'.format(valid_loss_min, valid_loss))\n","        torch.save(model.state_dict(), 'model_cifar.pt')\n","        valid_loss_min = valid_loss\n","    \n","    # ========================= plot ==========================\n","    plt.figure(figsize=(12, 4))\n","    plt.subplot(121)\n","    plt.plot(train_epochs_acc, '-o', label=\"train_acc\")\n","    plt.plot(valid_epochs_acc, '-o', label=\"valid_acc\")\n","    plt.title(\"Accuracy Graph by Epoch\")\n","    plt.legend()\n","    plt.subplot(122)\n","    plt.plot(train_epochs_loss, '-o', label=\"train_loss\")\n","    plt.plot(valid_epochs_loss, '-o', label=\"valid_loss\")\n","    plt.title(\"Loss Graphy by Epoch\")\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T14:16:11.140543Z","iopub.status.busy":"2023-12-08T14:16:11.139774Z","iopub.status.idle":"2023-12-08T14:16:20.291691Z","shell.execute_reply":"2023-12-08T14:16:20.290713Z","shell.execute_reply.started":"2023-12-08T14:16:11.140510Z"},"trusted":true},"outputs":[],"source":["model.load_state_dict(torch.load('model_cifar.pt'))\n","cnf_matrix = torch.zeros(10, 10)\n","test_loss_sum = 0.0\n","test_acc_num = 0.0\n","# store the number of correct classified samples for each class\n","# and their total number\n","class_correct = list(0 for i in range(10))\n","class_total = list(0 for i in range(10))\n","# Store 10 sample well-classified images and their confidence value\n","correct_classfied_flags = [False for i in range(10)]\n","correct_classfied_samples = [i for i in range(10)]\n","correct_classfied_values = [0 for i in range(10)]\n","correct_classfied_preds = [0 for i in range(10)]\n","# Store 10 sample miss-classified images and their confidence value\n","miss_classfied_flags = [False for i in range(10)]\n","miss_classfied_samples = [i for i in range(10)]\n","miss_classfied_values = [0 for i in range(10)]\n","miss_classfied_preds = [0 for i in range(10)]\n","\n","model.eval()\n","for images, labels in tqdm(test_loader):\n","    images, labels = images.to(device), labels.to(device)\n","    with torch.no_grad():\n","        logits = model(images)\n","        loss = loss_fn(logits, labels)\n","    test_loss_sum += loss.item() * images.size(0)\n","    pred = torch.max(logits, dim=1)[1]\n","    correct_tensor = pred == labels\n","    for idx in range(batch_size):\n","        # idx is the index of each image in one batch to RETRIVE from\n","        label = labels.data[idx].item()\n","        # label is the corresponding order of that image ot STORE in\n","        class_correct[label] += correct_tensor[idx].item()\n","        class_total[label] += 1\n","        if correct_tensor[idx] and not correct_classfied_flags[label]:\n","            correct_classfied_samples[label] = images.data[idx]\n","            correct_classfied_values[label] = max(nn.functional.softmax(logits[idx], dim=0)).item()\n","            correct_classfied_preds[label] = pred[idx].item()\n","            correct_classfied_flags[label] = True\n","        elif not correct_tensor[idx] and not miss_classfied_flags[label]:\n","            miss_classfied_samples[label] = images.data[idx]\n","            miss_classfied_values[label] = max(nn.functional.softmax(logits[idx], dim=0)).item()\n","            miss_classfied_preds[label] = pred[idx].item()\n","            miss_classfied_flags[label] = True   \n","    test_acc_num += sum(correct_tensor)\n","    cnf_matrix = confusion_matrix(pred, labels, cnf_matrix)\n","\n","test_loss = test_loss_sum/len(test_loader.dataset)\n","test_acc = 100 * test_acc_num/len(test_loader.dataset)\n","print('Test Loss: {:.3f}, Test Acc: {:.1f}%'.format(test_loss, test_acc))\n","print('Test Accuracy by Class:')\n","for i in range(10):\n","    if class_total[i] > 0:\n","        print('{:8s}\\t {:.1f}% ({:d}/{:d})'.format(class_names[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (class_names[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T14:16:20.293933Z","iopub.status.busy":"2023-12-08T14:16:20.293590Z","iopub.status.idle":"2023-12-08T14:16:21.057771Z","shell.execute_reply":"2023-12-08T14:16:21.056830Z","shell.execute_reply.started":"2023-12-08T14:16:20.293907Z"},"trusted":true},"outputs":[],"source":["plot_class_samples(correct_classfied_samples, correct_classfied_preds, correct_classfied_values, title='Well-calssified Examples')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T14:16:21.059301Z","iopub.status.busy":"2023-12-08T14:16:21.058995Z","iopub.status.idle":"2023-12-08T14:16:21.802695Z","shell.execute_reply":"2023-12-08T14:16:21.801604Z","shell.execute_reply.started":"2023-12-08T14:16:21.059263Z"},"trusted":true},"outputs":[],"source":["plot_class_samples(miss_classfied_samples, miss_classfied_preds, miss_classfied_values, title='Miss-classified Examples')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T14:16:21.806033Z","iopub.status.busy":"2023-12-08T14:16:21.805029Z","iopub.status.idle":"2023-12-08T14:16:22.488924Z","shell.execute_reply":"2023-12-08T14:16:22.487887Z","shell.execute_reply.started":"2023-12-08T14:16:21.805994Z"},"trusted":true},"outputs":[],"source":["plot_confusion_matrix(cnf_matrix.numpy(), class_names, normalize=False, ax_display=False, title='Confusion Matrix')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
